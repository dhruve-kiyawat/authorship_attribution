{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from sacremoses import MosesTokenizer, MosesPunctNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''एक छोटे से गाँव में नंदन नाम का एक लड़का रहता था। वह बहुत ही जिज्ञासु और मेहनती था। नंदन का सपना था कि वह बड़ा होकर एक महान वैज्ञानिक बने और अपने देश का नाम रोशन करे।\n",
    "\n",
    "गाँव के लोग अक्सर उसकी इस महत्वाकांक्षा पर हँसते थे और कहते थे, 'गाँव में रहकर कोई वैज्ञानिक नहीं बन सकता। तुम्हें खेती-बाड़ी ही करनी चाहिए।'\n",
    "\n",
    "लेकिन नंदन को लोगों की बातों से कोई फर्क नहीं पड़ता था। वह अपनी किताबों में डूबा रहता और नए-नए प्रयोग करता। उसके पास कोई बड़ी प्रयोगशाला नहीं थी, लेकिन उसने अपने छोटे से कमरे में ही कई चीजें इकट्ठा कर ली थीं – जैसे पुराने बैटरी, टूटे-फूटे उपकरण, और कागज के बने मॉडल।'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of pnnctuation\n",
    "import re \n",
    "from spacy.lang.hi import STOP_WORDS as STOP_WORDS_HI\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    # Define Hindi punctuation marks\n",
    "    hindi_punctuation = \"।,;:؟!‘’“”-''–\"\n",
    "    # Create a regex pattern to match any of the defined punctuation marks\n",
    "    pattern = f\"[{re.escape(hindi_punctuation)}]\"\n",
    "\n",
    "    # Replace punctuation with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Normalize\n",
    "def normalize_text(text):\n",
    "    # Unicode Normalization\n",
    "    normalizer_factory = IndicNormalizerFactory()\n",
    "    normalizer = normalizer_factory.get_normalizer(\"hi\")\n",
    "    \n",
    "    # Remove special characters and punctuations\n",
    "    tokenizer = MosesTokenizer(lang='hi')\n",
    "    normalizer = MosesPunctNormalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # Tokenize and rejoin to remove unnecessary spaces\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_text(text):\n",
    "    # Word tokenization using Indic NLP\n",
    "    tokens = list(indic_tokenize.trivial_tokenize(text))\n",
    "    return tokens\n",
    "\n",
    "# Stopwords removal\n",
    "def stopword_remover(text):\n",
    "    non_stop_words = [word for word in text if word not in set(STOP_WORDS_HI) ]\n",
    "    return non_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Preprocessing : [('और', 5), ('में', 4), ('एक', 3), ('से', 3), ('नंदन', 3), ('का', 3), ('था।', 3), ('वह', 3), ('ही', 3), ('कोई', 3), ('नहीं', 3), ('छोटे', 2), ('नाम', 2), ('रहता', 2), ('वैज्ञानिक', 2)]\n",
      "After Preprocessing : [('गाँव', 3), ('नंदन', 3), ('छोटे', 2), ('नाम', 2), ('रहता', 2), ('वैज्ञानिक', 2), ('बने', 2), ('लड़का', 1), ('जिज्ञासु', 1), ('मेहनती', 1), ('सपना', 1), ('बड़ा', 1), ('होकर', 1), ('महान', 1), ('देश', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Before Preprocessing :\", Counter(text.split(\" \")).most_common(15))\n",
    "\n",
    "text = remove_punctuations(text)\n",
    "text = normalize_text(text)\n",
    "tokens = tokenize_text(text)\n",
    "tokens = stopword_remover(tokens)\n",
    "\n",
    "print(\"After Preprocessing :\", Counter(tokens).most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['छोटे',\n",
       " 'गाँव',\n",
       " 'नंदन',\n",
       " 'नाम',\n",
       " 'लड़का',\n",
       " 'रहता',\n",
       " 'जिज्ञासु',\n",
       " 'मेहनती',\n",
       " 'नंदन',\n",
       " 'सपना',\n",
       " 'बड़ा',\n",
       " 'होकर',\n",
       " 'महान',\n",
       " 'वैज्ञानिक',\n",
       " 'बने',\n",
       " 'देश',\n",
       " 'नाम',\n",
       " 'रोशन',\n",
       " 'करे',\n",
       " 'गाँव',\n",
       " 'लोग',\n",
       " 'अक्सर',\n",
       " 'उसकी',\n",
       " 'महत्वाकांक्षा',\n",
       " 'हँसते',\n",
       " 'गाँव',\n",
       " 'रहकर',\n",
       " 'वैज्ञानिक',\n",
       " 'बन',\n",
       " 'तुम्हें',\n",
       " 'खेतीबाड़ी',\n",
       " 'करनी',\n",
       " 'चाहिए',\n",
       " 'नंदन',\n",
       " 'लोगों',\n",
       " 'बातों',\n",
       " 'फर्क',\n",
       " 'पड़ता',\n",
       " 'किताबों',\n",
       " 'डूबा',\n",
       " 'रहता',\n",
       " 'नएनए',\n",
       " 'प्रयोग',\n",
       " 'पास',\n",
       " 'बड़ी',\n",
       " 'प्रयोगशाला',\n",
       " 'उसने',\n",
       " 'छोटे',\n",
       " 'कमरे',\n",
       " 'चीजें',\n",
       " 'इकट्ठा',\n",
       " 'ली',\n",
       " 'थीं',\n",
       " 'पुराने',\n",
       " 'बैटरी',\n",
       " 'टूटेफूटे',\n",
       " 'उपकरण',\n",
       " 'कागज',\n",
       " 'बने',\n",
       " 'मॉडल']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "एक छोटे से गाँव में नंदन नाम का एक लड़का रहता था वह बहुत ही जिज्ञासु और मेहनती था नंदन का सपना था कि वह बड़ा होकर एक महान वैज्ञानिक बने और अपने देश का नाम रोशन करे गाँव के लोग अक्सर उसकी इस महत्वाकांक्षा पर हँसते थे और कहते थे गाँव में रहकर कोई वैज्ञानिक नहीं बन सकता तुम्हें खेतीबाड़ी ही करनी चाहिए लेकिन नंदन को लोगों की बातों से कोई फर्क नहीं पड़ता था वह अपनी किताबों में डूबा रहता और नएनए प्रयोग करता उसके पास कोई बड़ी प्रयोगशाला नहीं थी लेकिन उसने अपने छोटे से कमरे में ही कई चीजें इकट्ठा कर ली थीं जैसे पुराने बैटरी टूटेफूटे उपकरण और कागज के बने मॉडल\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Grams (Bigrams):\n",
      "['अक सर' 'इकट टर' 'उपकरण गज' 'उसक महत' 'उसन कमर' 'कमर इकट' 'कर अक' 'कर मह'\n",
      " 'करन दन' 'गज बन' 'गश उसन' 'टर उपकरण' 'दन फर' 'दन लड' 'दन सपन' 'नएनए रय'\n",
      " 'पड रहत' 'फर पड' 'बड कर' 'बड रय' 'बन करन' 'बन डल' 'बन शन' 'मह बन'\n",
      " 'महत सत' 'रय गश' 'रय बड' 'रहकर बन' 'रहत नएनए' 'रहत हनत' 'लड रहत' 'शन कर'\n",
      " 'सत रहकर' 'सपन बड' 'सर उसक' 'हनत दन']\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create the N-gram model (e.g., bigrams)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer='word')\n",
    "X = vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "# Display the bigrams\n",
    "print(\"N-Grams (Bigrams):\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _pywrap_tf2: A dynamic link library (DLL) initialization routine failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      3\u001b[0m sequences \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]]\n\u001b[0;32m      4\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(sequences, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ocean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF2_BEHAVIOR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n",
      "File \u001b[1;32mc:\\Users\\ocean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\tf2.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools to help with the TensorFlow 2.0 transition.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module is meant for TensorFlow internal implementation, not for users of\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mthe TensorFlow library. For that see tf.compat instead.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable\u001b[39m():\n\u001b[0;32m     26\u001b[0m   \u001b[38;5;66;03m# Enables v2 behaviors.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tf2: A dynamic link library (DLL) initialization routine failed."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = [[1, 2, 3, 4], [5, 6]]\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=4)\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Grams:\n",
      "['अक सर' 'अक सर उसक' 'इकट टर' 'इकट टर उपकरण' 'उपकरण गज' 'उपकरण गज बन'\n",
      " 'उसक महत' 'उसक महत सत' 'उसन कमर' 'उसन कमर इकट' 'कमर इकट' 'कमर इकट टर'\n",
      " 'कर अक' 'कर अक सर' 'कर मह' 'कर मह बन' 'करन दन' 'करन दन फर' 'गज बन'\n",
      " 'गज बन डल' 'गश उसन' 'गश उसन कमर' 'टर उपकरण' 'टर उपकरण गज' 'दन फर'\n",
      " 'दन फर पड' 'दन लड' 'दन लड रहत' 'दन सपन' 'दन सपन बड' 'नएनए रय'\n",
      " 'नएनए रय बड' 'पड रहत' 'पड रहत नएनए' 'फर पड' 'फर पड रहत' 'बड कर'\n",
      " 'बड कर मह' 'बड रय' 'बड रय गश' 'बन करन' 'बन करन दन' 'बन डल' 'बन शन'\n",
      " 'बन शन कर' 'मह बन' 'मह बन शन' 'महत सत' 'महत सत रहकर' 'रय गश' 'रय गश उसन'\n",
      " 'रय बड' 'रय बड रय' 'रहकर बन' 'रहकर बन करन' 'रहत नएनए' 'रहत नएनए रय'\n",
      " 'रहत हनत' 'रहत हनत दन' 'लड रहत' 'लड रहत हनत' 'शन कर' 'शन कर अक' 'सत रहकर'\n",
      " 'सत रहकर बन' 'सपन बड' 'सपन बड कर' 'सर उसक' 'सर उसक महत' 'हनत दन'\n",
      " 'हनत दन सपन']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the N-gram model (e.g., bigrams and trigrams)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 3), analyzer='word')\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "# Get the N-grams\n",
    "ngrams = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"N-Grams:\")\n",
    "print(ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
