{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from preprocess import normalize_text, remove_punctuations, tokenize_text, stopword_remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = remove_punctuations(text)\n",
    "    text = normalize_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = stopword_remover(tokens)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "file_dir = r'data\\**\\*.txt'\n",
    "file_paths = glob.glob(file_dir)\n",
    "print(len(file_paths))\n",
    "\n",
    "def readfile(filename):\n",
    "  from functools import reduce\n",
    "  with open(filename, 'r', encoding='utf-8') as f:\n",
    "      lines = f.readlines()\n",
    "      text = reduce(lambda a, b : a + \" \" +str(b), lines)\n",
    "  return text\n",
    "\n",
    "textlist = []\n",
    "for file in file_paths:\n",
    "  textlist.append(readfile(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 6 24 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def assign_unique_ids(input_list):\n",
    "    unique_ids = {}\n",
    "    result = []\n",
    "    current_id = 0\n",
    "    \n",
    "    for element in input_list:\n",
    "        if element not in unique_ids:\n",
    "            unique_ids[element] = current_id\n",
    "            current_id += 1\n",
    "        result.append(unique_ids[element])\n",
    "    \n",
    "    return result\n",
    "\n",
    "X, y = [], []\n",
    "for text in textlist:\n",
    "    X.append(preprocess(text))\n",
    "\n",
    "y = assign_unique_ids([file_path.split('\\\\')[1] for file_path in file_paths])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify=y, shuffle = True)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    अख कड  अच अच  अच नक  अच लग  अच लगत  अद लत  अध पक  अन अन  अन दर  अन नय  \\\n",
      "0       0      0      0      0       2      0      0      0      0      0   \n",
      "1       0      0      1      0       0      0      0      0      0      0   \n",
      "2       0      0      0      0       0      1      4      0      0      3   \n",
      "3       0      0      2      0       0      0      0      0      2      0   \n",
      "4       2      0      0      1       1      0      0      1      0      0   \n",
      "5       0      5      0      0       0      0      0      0      0      0   \n",
      "6       0      0      0      0       0      0      0      0      0      0   \n",
      "7       0      1      0      0       0      0      0      0      0      0   \n",
      "8       0      0      1      0       0      0      0      0      0      0   \n",
      "9       0      1      3      0       1      0      0      0      1      0   \n",
      "10      0      0      1      1       0      0      0      0      0      0   \n",
      "11      0      1      0      1       1      0      0      1      1      0   \n",
      "12      0      0      0      0       0      0      0      0      0      0   \n",
      "13      0      0      3      0       0      0      0      0      0      0   \n",
      "14      2      0      0      1       1      0      0      1      0      0   \n",
      "15      0      0      1      0       0      0      0      0      0      0   \n",
      "16      0      0      2      0       0      3      0      0      0      0   \n",
      "17      0      0      0      0       0      0      0      0      0      1   \n",
      "18      0      0      0      1       0      0      1      0      0      0   \n",
      "19      0      0      1      0       0      0      0      0      0      0   \n",
      "20      0      0      0      0       0      0      0      0      0      0   \n",
      "21      0      0      3      0       0      0      0      1      2      0   \n",
      "22      0      0      0      1       1      0      0      0      1      0   \n",
      "23      0      0      0      0       0      0      0      0      0      0   \n",
      "\n",
      "    ...  हर कल  हर खड  हर गई  हर बड  हर भर  हर मन  हर रह  हल कर  हव जह  हव ख़र  \n",
      "0   ...      0      0      0      0      0      0      0      0      0      0  \n",
      "1   ...      0      0      1      0      0      1      0      0      0      5  \n",
      "2   ...      3      0      0      0      0      0      0      0      0      0  \n",
      "3   ...      0      1      2      0      0      0      0      0      0      0  \n",
      "4   ...      2      0      0      1      0      0      0      0      0      0  \n",
      "5   ...      0      0      0      0      0      0      0      0      0      0  \n",
      "6   ...      0      0      0      0      0      0      1      0      0      0  \n",
      "7   ...      2      0      0      0      2      1      0      0      0      0  \n",
      "8   ...      0      0      0      1      0      0      0      0      0      0  \n",
      "9   ...      1      0      1      0      0      1      1      1      0      0  \n",
      "10  ...      0      3      0      1      0      3      2      0      0      0  \n",
      "11  ...      0      1      0      0      0      0      1      0      0      0  \n",
      "12  ...      0      0      0      0      0      0      0      3      0      0  \n",
      "13  ...      0      0      0      0      1      0      1      0      6      0  \n",
      "14  ...      2      0      0      1      0      0      0      0      0      0  \n",
      "15  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "16  ...      2      0      0      0      0      0      0      0      0      0  \n",
      "17  ...      0      0      0      0      1      0      0      0      0      0  \n",
      "18  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "19  ...      0      0      0      0      0      1      0      0      0      0  \n",
      "20  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "21  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "22  ...      2      0      0      0      0      0      0      0      0      0  \n",
      "23  ...      0      0      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[24 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "# N-gram\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# frequecy, select k best, pca, tsne\n",
    "\n",
    "# Initialize CountVectorizer with n-gram range (e.g., 2-grams)\n",
    "ngram_range = (2, 3)  # You can adjust the range as needed\n",
    "vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=768)\n",
    "\n",
    "# Fit and transform the stories into n-gram features\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Convert the result to a DataFrame for better visualization\n",
    "df_ngram_features = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the n-gram features\n",
    "print(df_ngram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruv\\miniconda3\\envs\\temp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dhruv\\miniconda3\\envs\\temp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\dhruv\\miniconda3\\envs\\temp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "from feature_extraction import get_bert_embeddings_for_long_text\n",
    "\n",
    "\n",
    "embedding = get_bert_embeddings_for_long_text(X_train[0])\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
